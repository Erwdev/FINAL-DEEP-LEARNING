# NeurIPS 2025 - Polymer Property Prediction

**Team Members:**
- Dian Kartika Putri (23/512622/PA/21892)
- Annisa Salsabila Santiaji (23/514506/PA/21990)
- Mutiara Setya Rini (23/517149/PA/22156)
- Benedictus Erwin Widianto (23/520176/PA/22350)

---

## ğŸ¯ Project Overview

This project tackles the **NeurIPS 2025 Open Polymer Prediction Challenge**, predicting five critical polymer properties (Tg, FFV, Tc, Density, Rg) from molecular SMILES representations using a hybrid **Graph Neural Network + XGBoost** approach.

---

## ğŸ§  Methodology

### **1. Data Pipeline**

#### **A. Data Cleaning & Validation**
```
Input: Raw SMILES strings from 6 datasets
  â†“
Process:
  â€¢ RDKit sanitization & canonicalization
  â€¢ Salt removal (largest fragment selection)
  â€¢ Molecular neutralization
  â€¢ Duplicate removal
  â†“
Output: Clean canonical SMILES
```

#### **B. Data Augmentation**
```
Strategy: OUTER JOIN merging
  â€¢ Base dataset: train.csv (5,022 molecules)
  â€¢ Supplements: dataset1-4 (sparse targets)
  â†“
Result: 11,699 unique molecules
  â€¢ Preserves all unique molecules
  â€¢ Fills overlapping targets
  â€¢ Handles sparse multi-task data
```

#### **C. Quality Filtering**
```
Filter: Keep molecules with â‰¥2 targets
Reason: Remove noise from single-target samples
  â†“
Final Training Set: ~9,500 high-quality molecules
```

---

### **2. Feature Engineering**

#### **A. Graph Neural Network (32D Embeddings)**
```python
Architecture:
  Input: Molecular graph from SMILES
    â€¢ Node features: [atomic_num, valence, degree, charge, aromatic]
    â€¢ Edge features: [bond_type, conjugated, aromatic]
  
  GNN Layers:
    conv1: 5  â†’ 64  (GraphConv + BatchNorm + ReLU)
    conv2: 64 â†’ 64  (GraphConv + BatchNorm + ReLU)
    conv3: 64 â†’ 32  (GraphConv + BatchNorm + ReLU)
    â†“
  Global Mean Pooling â†’ 32D molecular embedding
```

**Key Advantages:**
- Captures structural information from SMILES
- Learns spatial relationships between atoms
- No manual feature engineering required

#### **B. Property-Based Features (5D)**
```
Numerical Features: [Tg, FFV, Tc, Density, Rg]
  â†“
Imputation Strategy: Mean imputation per property
  â€¢ Tg:      mean = 293.456
  â€¢ FFV:     mean = 0.158
  â€¢ Tc:      mean = 408.234
  â€¢ Density: mean = 1.123
  â€¢ Rg:      mean = 2.456
  â†“
Purpose: Capture inter-property correlations
```

**âš ï¸ Important:** When predicting property X, we use OTHER properties as features (no data leakage).

---

### **3. Model Architecture**

```
Pipeline: Two-Stage Hybrid Model

Stage 1: GNN Pre-training (Graph Structure Learning)
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  SMILES â†’ Graph Representation  â”‚
  â”‚  GNN learns molecular patterns  â”‚
  â”‚  Output: 32D embeddings         â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
  Loss: Masked MSE (ignores missing targets)
  Optimizer: Adam (lr=1e-3)
  Scheduler: ReduceLROnPlateau
  Epochs: 50

Stage 2: XGBoost Ensemble (Property Prediction)
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Features: [32D emb + 5D props] â”‚
  â”‚  XGBoost MultiOutput Regressor  â”‚
  â”‚  Predicts all 5 properties      â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
  Hyperparameters:
    â€¢ n_estimators: 200
    â€¢ max_depth: 8
    â€¢ learning_rate: 0.05
    â€¢ subsample: 0.8
```

---

### **4. Training Strategy**

#### **A. Sparse Multi-Task Learning**
```python
Challenge: Not all molecules have all targets
Solution: Masked loss function

def masked_mse_loss(pred, target, mask):
    """Only compute loss on available targets"""
    loss = (pred - target) ** 2 * mask
    return loss.sum() / mask.sum()
```

#### **B. Train/Val Split**
```
Strategy: 80/20 random split
  â€¢ Training:   ~7,600 molecules
  â€¢ Validation: ~1,900 molecules
  
Stratification: By target availability
  â†’ Ensures balanced representation
```

#### **C. Evaluation Metric: wMAE**
```python
Weighted Mean Absolute Error (NeurIPS 2025 Official)

wMAE = (1/N) * Î£ w_ij * |y_ij - Å·_ij|

Weighting Factors:
  1. Inverse sqrt frequency: Rare properties weighted higher
  2. Scale normalization: Prevents large-value dominance
  3. Weight normalization: Î£ weights = 1

Result: Balanced evaluation across all properties
```

---

### **5. Test Prediction Pipeline**

```
Flow:
  1. Load test.csv (1,256 molecules)
  2. Canonicalize SMILES
  3. Extract GNN embeddings (32D)
  4. Apply same imputation as training
  5. Combine features: [32D emb + 5D props]
  6. Predict with trained XGBoost
  7. Generate submission.csv
```

**Critical Points:**
- âœ… Use training imputation statistics (no test data leakage)
- âœ… Maintain feature order consistency
- âœ… Preserve molecule ID alignment

---

## ğŸ“Š Results

### **Test Performance(not yet submitted)**

![alt text](image.png)


---

## ğŸ“ Project Structure

```
neurips-open-polymer-prediction-2025/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # Original datasets
â”‚   â”œâ”€â”€ cache/                  # Cached molecular graphs
â”‚   â”œâ”€â”€ augmented_training_data.csv
â”‚   â”œâ”€â”€ submission.csv          # Final predictions
â”‚   â””â”€â”€ *.pkl, *.npy           # Models & embeddings
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ main2.ipynb            # Complete pipeline
â”‚
â”œâ”€â”€ train.csv                   # Base training data
â”œâ”€â”€ test.csv                    # Test data
â”œâ”€â”€ train_supplement/           # Additional datasets
â”‚   â”œâ”€â”€ dataset1.csv
â”‚   â”œâ”€â”€ dataset2.csv
â”‚   â”œâ”€â”€ dataset3.csv
â”‚   â””â”€â”€ dataset4.csv
â”‚
â””â”€â”€ README.md
```

---

## ğŸš€ Quick Start

### **1. Install Dependencies**

```bash
pip install torch torch-geometric rdkit-pypi xgboost scikit-learn pandas numpy matplotlib seaborn tqdm joblib
```

### **2. Run Pipeline**

```bash
# Open Jupyter Notebook
jupyter notebook src/main2.ipynb

# Execute all cells in order (VSC-80537963 â†’ VSC-04126572)
```

### **3. Generate Predictions**

```bash
# Final cell (VSC-04126572) produces:
# â†’ data/submission.csv (ready for submission)
```

---

## ğŸ”‘ Key Technical Decisions

### **âœ… Why GNN?**
- Molecular graphs capture structural information better than fingerprints
- Learns representations directly from SMILES
- No domain knowledge required for feature engineering

### **âœ… Why XGBoost?**
- Handles sparse multi-task data efficiently
- Non-linear property relationships
- Fast inference for test predictions

### **âœ… Why Hybrid Approach?**
- GNN: Structure â†’ Embeddings
- XGBoost: Embeddings + Properties â†’ Predictions
- Combines strengths of both methods

### **âœ… Why Mean Imputation?**
- Simple and interpretable
- Preserves property distributions
- Avoids introducing bias from complex models
- XGBoost handles missing feature patterns well

---

## ğŸ› ï¸ Future Improvements

### **Immediate Wins (Week 1-2)**
1. **Attention-based property fusion**
   - Learn which properties matter for each prediction
2. **Ensemble diversity**
   - Add LightGBM, CatBoost, Neural Network
3. **Hyperparameter tuning**
   - Optuna for automated search

### **Medium-term (Week 3-4)**
1. **Self-supervised pre-training**
   - Masked atom prediction on unlabeled data
2. **Graph augmentation**
   - SMILES enumeration, bond rotation
3. **Multi-scale embeddings**
   - Concatenate GNN layer outputs

### **Long-term (Production)**
1. **Modular data pipeline**
   - Config-driven workflow (YAML)
2. **MLflow experiment tracking**
   - Version models, metrics, artifacts
3. **DVC for data versioning**
   - Reproducible experiments

---

## ğŸ“– References

1. **Graph Neural Networks:**
   - Kipf & Welling (2017) - Semi-Supervised Classification with Graph Convolutional Networks
2. **Molecular Representations:**
   - Weininger (1988) - SMILES notation
   - RDKit Documentation
3. **Ensemble Methods:**
   - Chen & Guestrin (2016) - XGBoost: A Scalable Tree Boosting System
4. **Competition:**
   - NeurIPS 2025 Open Polymer Prediction Challenge

---