{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29428989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def load_datasets(base_path='e:\\\\SEMESTER 5\\\\DEEP LEARNING\\\\FINAL\\\\neurips-open-polymer-prediction-2025'):\n",
    "    \"\"\"\n",
    "    Load all datasets from the project directory.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_path : str\n",
    "        Base path to the project directory\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing all loaded datasets\n",
    "    \"\"\"\n",
    "    datasets = {}\n",
    "    \n",
    "    # Load main train dataset\n",
    "    train_path = os.path.join(base_path, 'train.csv')\n",
    "    datasets['train'] = pd.read_csv(train_path)\n",
    "    # Load main test dataset\n",
    "    test_path = os.path.join(base_path, 'test.csv')\n",
    "    datasets['test'] = pd.read_csv(test_path)\n",
    "    \n",
    "    # Load supplementary datasets\n",
    "    supplement_path = os.path.join(base_path, 'train_supplement')\n",
    "    for i in range(1, 5):\n",
    "        dataset_path = os.path.join(supplement_path, f'dataset{i}.csv')\n",
    "        datasets[f'dataset{i}'] = pd.read_csv(dataset_path)\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "\n",
    "data = load_datasets()\n",
    "test_df = data['test']\n",
    "train_df = data['train']\n",
    "dataset1_df = data['dataset1']\n",
    "dataset2_df = data['dataset2']\n",
    "dataset3_df = data['dataset3']\n",
    "dataset4_df = data['dataset4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8ede9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem \n",
    "from rdkit.Chem import AllChem \n",
    "from rdkit.Chem.rdmolops import RemoveHs\n",
    "\n",
    "def safe_mol_from_smiles(smiles):\n",
    "    if pd.isna(smiles) or not isinstance(smiles, str) or smiles.strip() == '':\n",
    "        return None\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        return mol\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def largest_fragment(mol):\n",
    "    frags = Chem.GetMolFrags(mol, asMols=True)\n",
    "    if len(frags) == 1:\n",
    "        return mol\n",
    "    largest = max(frags, key=lambda m: m.GetNumAtoms())\n",
    "    return largest\n",
    "\n",
    "def neutralize_molecule(mol):\n",
    "    try:\n",
    "        # RDKit built-in neutralization\n",
    "        uncharger = Chem.rdMolStandardize.Uncharger()\n",
    "        mol = uncharger.uncharge(mol)\n",
    "        return mol\n",
    "    except:\n",
    "        return mol\n",
    "    \n",
    "def canonicalize(mol):\n",
    "    try:\n",
    "        return Chem.MolToSmiles(mol, canonical=True)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "def sanitize_smiles_pipeline(smiles):\n",
    "    # Step 1: Safe Mol\n",
    "    mol = safe_mol_from_smiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "\n",
    "    # Step 2: Remove salts\n",
    "    mol = largest_fragment(mol)\n",
    "\n",
    "    # Step 3: Neutralize\n",
    "    mol = neutralize_molecule(mol)\n",
    "\n",
    "    # Step 4: Re-sanitize\n",
    "    try:\n",
    "        Chem.SanitizeMol(mol)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    # Step 5: Canonicalize\n",
    "    return canonicalize(mol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35473667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====================================\n",
      "üîé QC for test_df\n",
      "=====================================\n",
      "Total entries:                  3\n",
      "Valid sanitized molecules:      3\n",
      "Invalid molecules:              0\n",
      "Raw unique SMILES:              3\n",
      "Canonical unique SMILES:        3\n",
      "Duplicates lost after cleaning: 0\n",
      "-------------------------------------\n",
      "‚úÖ No invalid molecules.\n",
      "\n",
      "=====================================\n",
      "üîé QC for train_df\n",
      "=====================================\n",
      "Total entries:                  7973\n",
      "Valid sanitized molecules:      7973\n",
      "Invalid molecules:              0\n",
      "Raw unique SMILES:              7973\n",
      "Canonical unique SMILES:        7973\n",
      "Duplicates lost after cleaning: 0\n",
      "-------------------------------------\n",
      "‚úÖ No invalid molecules.\n",
      "\n",
      "=====================================\n",
      "üîé QC for dataset1_df\n",
      "=====================================\n",
      "Total entries:                  874\n",
      "Valid sanitized molecules:      874\n",
      "Invalid molecules:              0\n",
      "Raw unique SMILES:              867\n",
      "Canonical unique SMILES:        866\n",
      "Duplicates lost after cleaning: 1\n",
      "-------------------------------------\n",
      "‚úÖ No invalid molecules.\n",
      "\n",
      "=====================================\n",
      "üîé QC for dataset2_df\n",
      "=====================================\n",
      "Total entries:                  7208\n",
      "Valid sanitized molecules:      7208\n",
      "Invalid molecules:              0\n",
      "Raw unique SMILES:              7174\n",
      "Canonical unique SMILES:        7174\n",
      "Duplicates lost after cleaning: 0\n",
      "-------------------------------------\n",
      "‚úÖ No invalid molecules.\n",
      "\n",
      "=====================================\n",
      "üîé QC for dataset3_df\n",
      "=====================================\n",
      "Total entries:                  46\n",
      "Valid sanitized molecules:      46\n",
      "Invalid molecules:              0\n",
      "Raw unique SMILES:              46\n",
      "Canonical unique SMILES:        46\n",
      "Duplicates lost after cleaning: 0\n",
      "-------------------------------------\n",
      "‚úÖ No invalid molecules.\n",
      "\n",
      "=====================================\n",
      "üîé QC for dataset4_df\n",
      "=====================================\n",
      "Total entries:                  862\n",
      "Valid sanitized molecules:      862\n",
      "Invalid molecules:              0\n",
      "Raw unique SMILES:              862\n",
      "Canonical unique SMILES:        862\n",
      "Duplicates lost after cleaning: 0\n",
      "-------------------------------------\n",
      "‚úÖ No invalid molecules.\n"
     ]
    }
   ],
   "source": [
    "# Replace your validate_smiles_entries call with this:\n",
    "\n",
    "def validate_smiles_entries_dict(dfs_dict, smiles_column=\"SMILES\"):\n",
    "    \"\"\"\n",
    "    SMILES validation using explicit dictionary keys.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for df_name, df in dfs_dict.items():\n",
    "        df_copy = df.copy()\n",
    "\n",
    "        canonical_list = []\n",
    "        errors = []\n",
    "\n",
    "        for s in df_copy[smiles_column]:\n",
    "            try:\n",
    "                mol = Chem.MolFromSmiles(s, sanitize=True)\n",
    "                if mol is None:\n",
    "                    canonical_list.append(None)\n",
    "                    errors.append({\"SMILES\": s, \"error\": \"MolFromSmiles returned None\"})\n",
    "                else:\n",
    "                    canon = Chem.MolToSmiles(mol, canonical=True)\n",
    "                    canonical_list.append(canon)\n",
    "                    errors.append(None)\n",
    "            except Exception as e:\n",
    "                canonical_list.append(None)\n",
    "                errors.append({\"SMILES\": s, \"error\": str(e)})\n",
    "\n",
    "        df_copy[\"canonical_SMILES\"] = canonical_list\n",
    "        df_copy[\"sanitization_error\"] = errors\n",
    "\n",
    "        # Summary QC\n",
    "        total = len(df_copy)\n",
    "        valid = df_copy[\"canonical_SMILES\"].notna().sum()\n",
    "        invalid = total - valid\n",
    "        unique_raw = df_copy[smiles_column].nunique()\n",
    "        unique_canon = df_copy[\"canonical_SMILES\"].nunique()\n",
    "\n",
    "        print(\"\\n=====================================\")\n",
    "        print(f\"üîé QC for {df_name}\")\n",
    "        print(\"=====================================\")\n",
    "        print(f\"Total entries:                  {total}\")\n",
    "        print(f\"Valid sanitized molecules:      {valid}\")\n",
    "        print(f\"Invalid molecules:              {invalid}\")\n",
    "        print(f\"Raw unique SMILES:              {unique_raw}\")\n",
    "        print(f\"Canonical unique SMILES:        {unique_canon}\")\n",
    "        print(f\"Duplicates lost after cleaning: {unique_raw - unique_canon}\")\n",
    "        print(\"-------------------------------------\")\n",
    "\n",
    "        invalid_df = df_copy[df_copy[\"canonical_SMILES\"].isna()]\n",
    "        if len(invalid_df) > 0:\n",
    "            print(\"‚ùå Invalid molecules found:\")\n",
    "            print(invalid_df[[smiles_column, \"sanitization_error\"]].head())\n",
    "        else:\n",
    "            print(\"‚úÖ No invalid molecules.\")\n",
    "\n",
    "        results[df_name] = {\n",
    "            \"clean_df\": df_copy[df_copy[\"canonical_SMILES\"].notna()].copy(),\n",
    "            \"invalid_df\": invalid_df.copy(),\n",
    "            \"full_df\": df_copy,\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "# Use explicit dictionary with names\n",
    "checklist = {\n",
    "    'test_df': test_df,\n",
    "    'train_df': train_df,\n",
    "    'dataset1_df': dataset1_df,\n",
    "    'dataset2_df': dataset2_df,\n",
    "    'dataset3_df': dataset3_df,\n",
    "    'dataset4_df': dataset4_df\n",
    "}\n",
    "\n",
    "qc_results = validate_smiles_entries_dict(checklist)\n",
    "\n",
    "# Now extract cleaned DataFrames\n",
    "train_df = qc_results['train_df']['clean_df']\n",
    "dataset1_df = qc_results['dataset1_df']['clean_df']\n",
    "dataset2_df = qc_results['dataset2_df']['clean_df']\n",
    "dataset3_df = qc_results['dataset3_df']['clean_df']\n",
    "dataset4_df = qc_results['dataset4_df']['clean_df']\n",
    "test_df = qc_results['test_df']['clean_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "948f60e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Prepare] Rows before=7973, after deduplicate=7973\n",
      "[Prepare] Rows before=874, after deduplicate=866\n",
      "[Prepare] Rows before=46, after deduplicate=46\n",
      "[Prepare] Rows before=862, after deduplicate=862\n",
      "\n",
      "===== BEFORE MERGE STATS =====\n",
      "Dataset 1: rows=7973, unique keys=7973\n",
      "Dataset 2: rows=866, unique keys=866\n",
      "Dataset 3: rows=46, unique keys=46\n",
      "Dataset 4: rows=862, unique keys=862\n",
      "\n",
      "===== AFTER MERGE STATS =====\n",
      "Merged rows=8972\n",
      "Merged unique keys=8972\n",
      "[Prepare] Rows before=8972, after deduplicate=8972\n",
      "[Prepare] Rows before=7208, after deduplicate=7174\n",
      "\n",
      "===== BEFORE MERGE STATS =====\n",
      "Dataset 1: rows=8972, unique keys=8972\n",
      "Dataset 2: rows=7174, unique keys=7174\n",
      "\n",
      "===== AFTER MERGE STATS =====\n",
      "Merged rows=10343\n",
      "Merged unique keys=10343\n",
      "\n",
      "===== AUGMENTATION DIAGNOSTICS =====\n",
      "New molecules added: 1371\n",
      "Molecules missing (should be 0): 0\n",
      "Example added rows:\n",
      "   id  Tg  FFV  Tc  Density  Rg  \\\n",
      "0 NaN NaN  NaN NaN      NaN NaN   \n",
      "1 NaN NaN  NaN NaN      NaN NaN   \n",
      "2 NaN NaN  NaN NaN      NaN NaN   \n",
      "3 NaN NaN  NaN NaN      NaN NaN   \n",
      "4 NaN NaN  NaN NaN      NaN NaN   \n",
      "\n",
      "                                    canonical_SMILES sanitization_error  \\\n",
      "0              */C(=C(/*)c1ccc(C(C)(C)C)cc1)c1ccccc1                NaN   \n",
      "1                  */C(=C(/*)c1ccc(CCCC)cc1)c1ccccc1                NaN   \n",
      "2             */C(=C(/*)c1ccc(Oc2ccccc2)cc1)c1ccccc1                NaN   \n",
      "3  */C(=C(/*)c1ccc([Si](C(C)C)(C(C)C)C(C)C)cc1)c1...                NaN   \n",
      "4           */C(=C(/*)c1ccc([Si](C)(C)C)cc1)c1ccccc1                NaN   \n",
      "\n",
      "   TC_mean  \n",
      "0      NaN  \n",
      "1      NaN  \n",
      "2      NaN  \n",
      "3      NaN  \n",
      "4      NaN  \n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Tg",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "FFV",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Tc",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Density",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Rg",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "canonical_SMILES",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "TC_mean",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "2cf389c9-7273-4e7b-b891-3c316535fd6a",
       "rows": [
        [
         "0",
         null,
         null,
         null,
         null,
         null,
         "*/C(=C(/*)c1ccc(C(C)(C)C)cc1)c1ccccc1",
         null
        ],
        [
         "1",
         null,
         null,
         null,
         null,
         null,
         "*/C(=C(/*)c1ccc(CCCC)cc1)c1ccccc1",
         null
        ],
        [
         "2",
         null,
         null,
         null,
         null,
         null,
         "*/C(=C(/*)c1ccc(Oc2ccccc2)cc1)c1ccccc1",
         null
        ],
        [
         "3",
         null,
         null,
         null,
         null,
         null,
         "*/C(=C(/*)c1ccc([Si](C(C)C)(C(C)C)C(C)C)cc1)c1ccccc1",
         null
        ],
        [
         "4",
         null,
         null,
         null,
         null,
         null,
         "*/C(=C(/*)c1ccc([Si](C)(C)C)cc1)c1ccccc1",
         null
        ],
        [
         "5",
         null,
         null,
         null,
         null,
         null,
         "*/C(=C(/*)c1ccc[c]([Ge]([CH3])([CH3])[CH3])c1)c1ccccc1",
         null
        ],
        [
         "6",
         null,
         null,
         null,
         null,
         null,
         "*/C(=C(/*)c1cccc([Si](C)(C)C)c1)c1ccccc1",
         null
        ],
        [
         "7",
         "206.5698859",
         null,
         null,
         null,
         null,
         "*/C(=C(/*)c1ccccc1)c1ccccc1",
         null
        ],
        [
         "8",
         null,
         null,
         null,
         null,
         null,
         "*/C(=C(/c1ccc(*)cc1)c1ccc(Oc2ccccc2)cc1)c1ccc(Oc2ccccc2)cc1",
         null
        ],
        [
         "9",
         null,
         null,
         null,
         null,
         null,
         "*/C(=C(\\C#N)c1ccc(C(=O)OC2CCN(*)CC2)cc1)c1ccc(OC)cc1",
         null
        ],
        [
         "10",
         null,
         null,
         null,
         null,
         null,
         "*/C(=C(\\[2H])C([2H])([2H])C(*)([2H])[2H])C([2H])([2H])[2H]",
         null
        ],
        [
         "11",
         null,
         null,
         null,
         null,
         null,
         "*/C(=C(\\c1ccccc1)c1ccc(*)cc1)c1ccccc1",
         "0.338"
        ],
        [
         "12",
         null,
         null,
         null,
         null,
         null,
         "*/C(=C/c1cc(OC)c(/C=C(/c2ccc(*)cc2)c2ccc(C(F)(F)F)cc2)cc1OC)c1ccc(C(F)(F)F)cc1",
         null
        ],
        [
         "13",
         null,
         null,
         null,
         null,
         null,
         "*/C(=C/c1cc(OC)c(/C=C(/c2ccc(*)cc2)c2ccc(F)cc2)cc1OC)c1ccc(F)cc1",
         null
        ],
        [
         "14",
         null,
         null,
         null,
         null,
         null,
         "*/C(=C/c1cc(OC)c(/C=C(/c2ccc(*)cc2)c2ccc(OC)cc2)cc1OC)c1ccc(OC)cc1",
         null
        ],
        [
         "15",
         null,
         null,
         null,
         null,
         null,
         "*/C(=C/c1cc(OC)c(/C=C(\\c2ccc(F)cc2)c2ccc(-c3ccc(*)cc3)cc2)cc1OC)c1ccc(F)cc1",
         null
        ],
        [
         "16",
         null,
         null,
         null,
         null,
         null,
         "*/C(=C/c1cc(OC)c(/C=C(\\c2ccc(F)cc2)c2ccc3cc(*)ccc3c2)cc1OC)c1ccc(F)cc1",
         null
        ],
        [
         "17",
         null,
         null,
         null,
         null,
         null,
         "*/C(=C/c1cc(OC)c(/C=C(\\c2ccccc2)c2ccc(-c3ccc(*)cc3)cc2)cc1OC)c1ccccc1",
         null
        ],
        [
         "18",
         null,
         null,
         null,
         null,
         null,
         "*/C(=C/c1cc(OC)c(/C=C(\\c2ccccc2)c2ccc3cc(*)ccc3c2)cc1OC)c1ccccc1",
         null
        ],
        [
         "19",
         null,
         null,
         null,
         null,
         null,
         "*/C(=C/c1cc(OCCCCCCCC)c(/C=C(\\c2ccccc2)c2ccc3c(c2)Sc2ccc(*)cc2S3)cc1OCCCCCCCC)c1ccccc1",
         null
        ],
        [
         "20",
         null,
         null,
         null,
         null,
         null,
         "*/C(=C/c1ccc(/C=C(\\c2ccccc2)c2ccc(-c3ccc(*)cc3)cc2)cc1)c1ccccc1",
         null
        ],
        [
         "21",
         null,
         null,
         null,
         null,
         null,
         "*/C(=N\\c1ccccc1)c1ccc(-c2ccc(*)cc2)cc1",
         null
        ],
        [
         "22",
         null,
         null,
         null,
         null,
         null,
         "*/C(C#N)=C(\\C#N)N1C(=O)c2ccc(C(=O)c3ccc(-c4ccc5c(c4)C(=O)N(*)C5=O)cc3)cc2C1=O",
         null
        ],
        [
         "23",
         null,
         null,
         null,
         null,
         null,
         "*/C(C#N)=C(\\C#N)n1c(=O)c2cc3c(=O)n(*)c(=O)c3cc2c1=O",
         null
        ],
        [
         "24",
         null,
         null,
         null,
         null,
         null,
         "*/C(C#N)=C/c1cc(/C=C(\\C#N)c2ccc3c(c2)C(CCCCCCCC)(CCCCCCCC)c2cc(*)ccc2-3)cc(N(c2ccccc2)c2ccccc2)c1",
         null
        ],
        [
         "25",
         null,
         null,
         null,
         null,
         null,
         "*/C(C#N)=C/c1cc(OCC(CC)CCCC)c(/C=C(\\C#N)c2ccc3c4ccc(*)cc4n(CC(CC)CCCC)c3c2)cc1OCC(CC)CCCC",
         null
        ],
        [
         "26",
         null,
         null,
         null,
         null,
         null,
         "*/C(C#N)=C/c1cc(OCCCCCCCC)c(/C=C(\\C#N)c2cc(OCCCCCCCC)c(*)cc2OC)cc1OC",
         null
        ],
        [
         "27",
         null,
         null,
         null,
         null,
         null,
         "*/C(C#N)=C/c1cc(OCCCCCCCCCC)c(/C=C(\\C#N)c2cc(OCCCCCCCCCC)c(*)cc2OC)cc1OC",
         null
        ],
        [
         "28",
         null,
         null,
         null,
         null,
         null,
         "*/C(C#N)=C/c1cc(OCCCCCCCCCCCC)c(/C=C(\\C#N)c2cc(OCCCCCCCCCCCC)c(*)cc2OC)cc1OC",
         null
        ],
        [
         "29",
         null,
         null,
         null,
         null,
         null,
         "*/C(C#N)=C/c1cc(OCCCCCCCCCCCCCCCC)c(/C=C(\\C#N)c2cc(OCCCCCCCCCCCCCCCC)c(*)cc2OC)cc1OC",
         null
        ],
        [
         "30",
         null,
         null,
         null,
         null,
         null,
         "*/C(C#N)=C/c1ccc(N(CC)Cc2ccccc2CN(CC)c2ccc(/C=C(\\C#N)S(*)(=O)=O)cc2)cc1",
         null
        ],
        [
         "31",
         null,
         null,
         null,
         null,
         null,
         "*/C(C)=C(/*)CCC",
         null
        ],
        [
         "32",
         null,
         null,
         null,
         null,
         null,
         "*/C(C)=C(/*)CCCCC",
         null
        ],
        [
         "33",
         null,
         null,
         null,
         null,
         null,
         "*/C(C)=C(/*)CCCCCCC",
         null
        ],
        [
         "34",
         null,
         null,
         null,
         null,
         null,
         "*/C(C)=C(/*)SCC",
         null
        ],
        [
         "35",
         null,
         null,
         null,
         null,
         null,
         "*/C(C)=C(/*)SCCCCCC",
         null
        ],
        [
         "36",
         null,
         null,
         null,
         null,
         null,
         "*/C(C)=C(/*)SCCCCCCCCCC",
         null
        ],
        [
         "37",
         null,
         null,
         null,
         null,
         null,
         "*/C(C)=C(/*)[Si](C)(C)C",
         null
        ],
        [
         "38",
         null,
         null,
         null,
         null,
         null,
         "*/C(C)=C(/*)[Si](C)(C)CCCCCC",
         null
        ],
        [
         "39",
         null,
         null,
         null,
         null,
         null,
         "*/C(C)=C(/*)[Si](C)(C)CC[Si](C)(C)C",
         null
        ],
        [
         "40",
         null,
         null,
         null,
         null,
         null,
         "*/C(C)=C(/*)[Si](C)(C)C[Si](C)(C)C",
         null
        ],
        [
         "41",
         null,
         null,
         null,
         null,
         null,
         "*/C(C)=C(/*)c1ccc([Si](C)(C)C)cc1",
         null
        ],
        [
         "42",
         null,
         null,
         null,
         null,
         null,
         "*/C(C)=[C](/*)[Ge]([CH3])([CH3])[CH3]",
         null
        ],
        [
         "43",
         null,
         null,
         null,
         null,
         null,
         "*/C(CC)=C(/*)c1ccccc1",
         null
        ],
        [
         "44",
         null,
         null,
         null,
         null,
         null,
         "*/C(CCCCCC)=C(/*)c1ccccc1",
         null
        ],
        [
         "45",
         null,
         null,
         null,
         null,
         null,
         "*/C(Cl)=C(/*)CCCC",
         null
        ],
        [
         "46",
         null,
         null,
         null,
         null,
         null,
         "*/C(Cl)=C(/*)CCCCCC",
         null
        ],
        [
         "47",
         null,
         null,
         null,
         null,
         null,
         "*/C(Cl)=C(/*)CCCCCCCC",
         null
        ],
        [
         "48",
         null,
         null,
         null,
         null,
         null,
         "*/C(Cl)=C(/*)c1ccccc1",
         null
        ],
        [
         "49",
         null,
         null,
         "0.102",
         null,
         null,
         "*/C(F)=C(\\F)C(F)(C(*)(F)F)C(F)(F)F",
         "0.102"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 10343
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tg</th>\n",
       "      <th>FFV</th>\n",
       "      <th>Tc</th>\n",
       "      <th>Density</th>\n",
       "      <th>Rg</th>\n",
       "      <th>canonical_SMILES</th>\n",
       "      <th>TC_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>*/C(=C(/*)c1ccc(C(C)(C)C)cc1)c1ccccc1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>*/C(=C(/*)c1ccc(CCCC)cc1)c1ccccc1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>*/C(=C(/*)c1ccc(Oc2ccccc2)cc1)c1ccccc1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>*/C(=C(/*)c1ccc([Si](C(C)C)(C(C)C)C(C)C)cc1)c1...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>*/C(=C(/*)c1ccc([Si](C)(C)C)cc1)c1ccccc1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10338</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.38800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>*c1sc(*)c(OCCCCCCCCCCCCCCCC)c1C</td>\n",
       "      <td>0.38800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10339</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.37475</td>\n",
       "      <td>0.904355</td>\n",
       "      <td>14.348260</td>\n",
       "      <td>*c1sc(*)c(OCCCCCCCCCCCCCCCCCCCC)c1C</td>\n",
       "      <td>0.37475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10340</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.44475</td>\n",
       "      <td>0.968872</td>\n",
       "      <td>15.087862</td>\n",
       "      <td>*c1sc(*)c2c1OCC(CCCCCCCCCCCCCCCC)O2</td>\n",
       "      <td>0.44475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10341</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>*c1sc(*)c2sc(CCCCCCCCC)nc12</td>\n",
       "      <td>0.48200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10342</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>*c1sc(-c2cc(CCCCCCCCCC)c(*)s2)cc1CCCCCCCCCC</td>\n",
       "      <td>0.30700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10343 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Tg  FFV       Tc   Density         Rg  \\\n",
       "0     NaN  NaN      NaN       NaN        NaN   \n",
       "1     NaN  NaN      NaN       NaN        NaN   \n",
       "2     NaN  NaN      NaN       NaN        NaN   \n",
       "3     NaN  NaN      NaN       NaN        NaN   \n",
       "4     NaN  NaN      NaN       NaN        NaN   \n",
       "...    ..  ...      ...       ...        ...   \n",
       "10338 NaN  NaN  0.38800       NaN        NaN   \n",
       "10339 NaN  NaN  0.37475  0.904355  14.348260   \n",
       "10340 NaN  NaN  0.44475  0.968872  15.087862   \n",
       "10341 NaN  NaN      NaN       NaN        NaN   \n",
       "10342 NaN  NaN      NaN       NaN        NaN   \n",
       "\n",
       "                                        canonical_SMILES  TC_mean  \n",
       "0                  */C(=C(/*)c1ccc(C(C)(C)C)cc1)c1ccccc1      NaN  \n",
       "1                      */C(=C(/*)c1ccc(CCCC)cc1)c1ccccc1      NaN  \n",
       "2                 */C(=C(/*)c1ccc(Oc2ccccc2)cc1)c1ccccc1      NaN  \n",
       "3      */C(=C(/*)c1ccc([Si](C(C)C)(C(C)C)C(C)C)cc1)c1...      NaN  \n",
       "4               */C(=C(/*)c1ccc([Si](C)(C)C)cc1)c1ccccc1      NaN  \n",
       "...                                                  ...      ...  \n",
       "10338                    *c1sc(*)c(OCCCCCCCCCCCCCCCC)c1C  0.38800  \n",
       "10339                *c1sc(*)c(OCCCCCCCCCCCCCCCCCCCC)c1C  0.37475  \n",
       "10340                *c1sc(*)c2c1OCC(CCCCCCCCCCCCCCCC)O2  0.44475  \n",
       "10341                        *c1sc(*)c2sc(CCCCCCCCC)nc12  0.48200  \n",
       "10342        *c1sc(-c2cc(CCCCCCCCCC)c(*)s2)cc1CCCCCCCCCC  0.30700  \n",
       "\n",
       "[10343 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_merge = [train_df, dataset1_df, dataset3_df, dataset4_df]\n",
    "dataset_cluster = [dataset2_df]\n",
    "\n",
    "def prepare_for_merge(df, key=\"canonical_SMILES\"):\n",
    "    \"\"\"\n",
    "    Remove duplicate SMILES columns and preserve only the key for merging.\n",
    "    Keeps track of original row count and unique keys.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Drop raw SMILES if canonical is used for merge\n",
    "    if \"SMILES\" in df.columns and key != \"SMILES\":\n",
    "        df = df.drop(columns=[\"SMILES\"])\n",
    "\n",
    "    # Drop duplicate key rows\n",
    "    before = len(df)\n",
    "    df = df.drop_duplicates(subset=[key])\n",
    "    after = len(df)\n",
    "\n",
    "    print(f\"[Prepare] Rows before={before}, after deduplicate={after}\")\n",
    "    return df\n",
    "from functools import reduce\n",
    "\n",
    "def robust_merge(datasets, key=\"canonical_SMILES\"):\n",
    "    \"\"\"\n",
    "    Safely merges multiple datasets on canonical SMILES.\n",
    "    Logs before/after information for traceability.\n",
    "    \"\"\"\n",
    "    # Prepare datasets\n",
    "    cleaned = [prepare_for_merge(df, key=key) for df in datasets]\n",
    "\n",
    "    # Log BEFORE merge stats\n",
    "    print(\"\\n===== BEFORE MERGE STATS =====\")\n",
    "    for i, df in enumerate(cleaned):\n",
    "        print(f\"Dataset {i+1}: rows={len(df)}, unique keys={df[key].nunique()}\")\n",
    "\n",
    "    # Reduce merge\n",
    "    merged = reduce(\n",
    "        lambda left, right: left.merge(\n",
    "            right, on=key, how=\"outer\", suffixes=(\"\", \"_dup\")\n",
    "        ),\n",
    "        cleaned\n",
    "    )\n",
    "\n",
    "    # Clean leftover duplicate columns (_dup)\n",
    "    dup_cols = [c for c in merged.columns if c.endswith(\"_dup\")]\n",
    "    if dup_cols:\n",
    "        merged = merged.drop(columns=dup_cols)\n",
    "\n",
    "    # Log AFTER merge stats\n",
    "    print(\"\\n===== AFTER MERGE STATS =====\")\n",
    "    print(f\"Merged rows={len(merged)}\")\n",
    "    print(f\"Merged unique keys={merged[key].nunique()}\")\n",
    "\n",
    "    return merged\n",
    "\n",
    "\n",
    "def compare_augmentation(base_df, augmented_df, key=\"canonical_SMILES\"):\n",
    "    \"\"\"\n",
    "    Show which molecules the external datasets added.\n",
    "    \"\"\"\n",
    "    base_keys = set(base_df[key])\n",
    "    aug_keys = set(augmented_df[key])\n",
    "\n",
    "    added = aug_keys - base_keys\n",
    "    lost  = base_keys - aug_keys  # should be empty unless outer merge\n",
    "\n",
    "    print(\"\\n===== AUGMENTATION DIAGNOSTICS =====\")\n",
    "    print(f\"New molecules added: {len(added)}\")\n",
    "    print(f\"Molecules missing (should be 0): {len(lost)}\")\n",
    "\n",
    "    added_df = augmented_df[augmented_df[key].isin(added)]\n",
    "    print(\"Example added rows:\")\n",
    "    print(added_df.head())\n",
    "\n",
    "    return added_df\n",
    "\n",
    "df_internal = robust_merge(dataset_merge, key=\"canonical_SMILES\")\n",
    "df_augmented = robust_merge([df_internal] + dataset_cluster)\n",
    "\n",
    "augment_inspection = compare_augmentation(df_internal, df_augmented)\n",
    "\n",
    "df_augmented = df_augmented.drop(columns=[\"sanitization_error\",\"id\"])\n",
    "df_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "058ff07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìä AVAILABLE FEATURES IN AUGMENTED DATA\n",
      "============================================================\n",
      "\n",
      "All columns:\n",
      "['Tg', 'FFV', 'Tc', 'Density', 'Rg', 'canonical_SMILES', 'TC_mean']\n",
      "\n",
      "üìã Target columns (5):\n",
      "   - Tg\n",
      "   - FFV\n",
      "   - Tc\n",
      "   - Density\n",
      "   - Rg\n",
      "\n",
      "üî¢ ALL Numerical feature columns (6):\n",
      "   - Tg                   ‚≠ê TARGET\n",
      "   - FFV                  ‚≠ê TARGET\n",
      "   - Tc                   ‚≠ê TARGET\n",
      "   - Density              ‚≠ê TARGET\n",
      "   - Rg                   ‚≠ê TARGET\n",
      "   - TC_mean              \n",
      "\n",
      "üìä Feature coverage (including NaN values):\n",
      "  [TARGET]   Tg                  :    511 /  10343 (  4.9%)\n",
      "  [TARGET]   FFV                 :   7030 /  10343 ( 68.0%)\n",
      "  [TARGET]   Tc                  :    737 /  10343 (  7.1%)\n",
      "  [TARGET]   Density             :    613 /  10343 (  5.9%)\n",
      "  [TARGET]   Rg                  :    614 /  10343 (  5.9%)\n",
      "  [FEATURE]  TC_mean             :    866 /  10343 (  8.4%)\n",
      "\n",
      "============================================================\n",
      "‚úÖ Combined features for XGBoost:\n",
      "   - GNN embeddings:      32 dimensions\n",
      "   - Target columns:      5 dimensions (will be used as features)\n",
      "   - Other numerical:     1 dimensions\n",
      "   - TOTAL:              38 dimensions\n",
      "============================================================\n",
      "\n",
      "‚ö†Ô∏è  NOTE: Target columns with available values will be used as features\n",
      "   Missing target values (NaN) will be filled with 0\n"
     ]
    }
   ],
   "source": [
    "# Identify available columns\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä AVAILABLE FEATURES IN AUGMENTED DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nAll columns:\")\n",
    "print(df_augmented.columns.tolist())\n",
    "\n",
    "# Define target columns (these are what we predict)\n",
    "target_columns = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "\n",
    "# Get ALL numerical columns EXCEPT canonical_SMILES\n",
    "# This includes target columns (they have numerical data too!)\n",
    "numerical_columns = []\n",
    "for col in df_augmented.columns:\n",
    "    if col != 'canonical_SMILES':  # Only exclude SMILES identifier\n",
    "        # Check if column is numeric (robust check)\n",
    "        if pd.api.types.is_numeric_dtype(df_augmented[col]):\n",
    "            numerical_columns.append(col)\n",
    "\n",
    "print(f\"\\nüìã Target columns ({len(target_columns)}):\")\n",
    "for col in target_columns:\n",
    "    print(f\"   - {col}\")\n",
    "\n",
    "print(f\"\\nüî¢ ALL Numerical feature columns ({len(numerical_columns)}):\")\n",
    "for col in numerical_columns:\n",
    "    is_target = \"‚≠ê TARGET\" if col in target_columns else \"\"\n",
    "    print(f\"   - {col:20s} {is_target}\")\n",
    "\n",
    "if len(numerical_columns) > 0:\n",
    "    print(f\"\\nüìä Feature coverage (including NaN values):\")\n",
    "    for col in numerical_columns:\n",
    "        available = df_augmented[col].notna().sum()\n",
    "        total = len(df_augmented)\n",
    "        pct = (available / total) * 100\n",
    "        is_target = \"[TARGET]\" if col in target_columns else \"[FEATURE]\"\n",
    "        print(f\"  {is_target:10s} {col:20s}: {available:6d} / {total:6d} ({pct:5.1f}%)\")\n",
    "    \n",
    "    # Count non-target features\n",
    "    non_target_features = [col for col in numerical_columns if col not in target_columns]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"‚úÖ Combined features for XGBoost:\")\n",
    "    print(f\"   - GNN embeddings:      32 dimensions\")\n",
    "    print(f\"   - Target columns:      {len(target_columns)} dimensions (will be used as features)\")\n",
    "    if len(non_target_features) > 0:\n",
    "        print(f\"   - Other numerical:     {len(non_target_features)} dimensions\")\n",
    "        print(f\"   - TOTAL:              {32 + len(numerical_columns)} dimensions\")\n",
    "    else:\n",
    "        print(f\"   - TOTAL:              {32 + len(target_columns)} dimensions\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è  NOTE: Target columns with available values will be used as features\")\n",
    "    print(f\"   Missing target values (NaN) will be filled with 0\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No numerical features found!\")\n",
    "    print(\"   Will use GNN embeddings only (32 dimensions)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e748570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "save_dir = \"../data\"  # relative dari src/\n",
    "save_path = os.path.join(save_dir, \"augmented_training_data.csv\")\n",
    "\n",
    "df_augmented.to_csv(save_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fd8d8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\dl_env\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "\n",
    "def get_atom_features(atom):\n",
    "    \"\"\"\n",
    "    Extract node features from RDKit atom object.\n",
    "    Features: atomic number, valence, degree, formal charge, aromaticity\n",
    "    \"\"\"\n",
    "    features = [\n",
    "        atom.GetAtomicNum(),                    # Atomic number\n",
    "        atom.GetTotalValence(),                 # Valence\n",
    "        atom.GetDegree(),                       # Degree\n",
    "        atom.GetFormalCharge(),                 # Formal charge\n",
    "        int(atom.GetIsAromatic())               # Aromaticity (0 or 1)\n",
    "    ]\n",
    "    return features\n",
    "\n",
    "def get_bond_features(bond):\n",
    "    \"\"\"\n",
    "    Extract edge features from RDKit bond object.\n",
    "    Features: bond type, conjugation, aromatic flags\n",
    "    \"\"\"\n",
    "    bond_type_map = {\n",
    "        Chem.BondType.SINGLE: 1,\n",
    "        Chem.BondType.DOUBLE: 2,\n",
    "        Chem.BondType.TRIPLE: 3,\n",
    "        Chem.BondType.AROMATIC: 4\n",
    "    }\n",
    "    \n",
    "    features = [\n",
    "        bond_type_map.get(bond.GetBondType(), 0),  # Bond type\n",
    "        int(bond.GetIsConjugated()),                # Conjugation\n",
    "        int(bond.GetIsAromatic())                   # Aromatic flag\n",
    "    ]\n",
    "    return features\n",
    "\n",
    "def smiles_to_graph(smiles):\n",
    "    \"\"\"\n",
    "    Convert SMILES string to PyTorch Geometric Data object.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Data object with:\n",
    "        - x: node feature matrix [num_nodes, 5]\n",
    "        - edge_index: edge connectivity [2, num_edges]\n",
    "        - edge_attr: edge features [num_edges, 3]\n",
    "    \"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    \n",
    "    # Node features\n",
    "    atom_features = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        atom_features.append(get_atom_features(atom))\n",
    "    x = torch.tensor(atom_features, dtype=torch.float)\n",
    "    \n",
    "    # Edge indices and features\n",
    "    edge_indices = []\n",
    "    edge_features = []\n",
    "    \n",
    "    for bond in mol.GetBonds():\n",
    "        i = bond.GetBeginAtomIdx()\n",
    "        j = bond.GetEndAtomIdx()\n",
    "        \n",
    "        # Add both directions (undirected graph)\n",
    "        edge_indices.append([i, j])\n",
    "        edge_indices.append([j, i])\n",
    "        \n",
    "        bond_feat = get_bond_features(bond)\n",
    "        edge_features.append(bond_feat)\n",
    "        edge_features.append(bond_feat)\n",
    "    \n",
    "    edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_features, dtype=torch.float)\n",
    "    \n",
    "    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Dataset, DataLoader, Batch\n",
    "import pandas as pd\n",
    "\n",
    "class PolymerDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for polymer property prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, target_columns=['Tg', 'FFV', 'Tc', 'Density', 'Rg']):\n",
    "        super().__init__()\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.target_columns = target_columns\n",
    "        self.smiles_column = 'canonical_SMILES'\n",
    "        \n",
    "        # Filter out rows with all NaN targets\n",
    "        valid_rows = self.df[target_columns].notna().any(axis=1)\n",
    "        self.df = self.df[valid_rows].reset_index(drop=True)\n",
    "        \n",
    "    def len(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def get(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        smiles = row[self.smiles_column]\n",
    "        \n",
    "        # Convert SMILES to graph\n",
    "        graph = smiles_to_graph(smiles)\n",
    "        if graph is None:\n",
    "            return None\n",
    "        \n",
    "        # Extract target values (handle NaN)\n",
    "        targets = []\n",
    "        target_mask = []\n",
    "        for col in self.target_columns:\n",
    "            val = row[col]\n",
    "            if pd.isna(val):\n",
    "                targets.append(0.0)\n",
    "                target_mask.append(0)\n",
    "            else:\n",
    "                targets.append(float(val))\n",
    "                target_mask.append(1)\n",
    "        \n",
    "        graph.y = torch.tensor([targets], dtype=torch.float)  # ‚Üê FIX: Add extra dimension\n",
    "        graph.y_mask = torch.tensor([target_mask], dtype=torch.float)  # ‚Üê FIX: Add extra dimension\n",
    "        \n",
    "        return graph\n",
    "\n",
    "def custom_collate(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle None values and properly batch graphs.\n",
    "    \"\"\"\n",
    "    # Filter out None values\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    \n",
    "    if len(batch) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Use PyTorch Geometric's Batch.from_data_list\n",
    "    return Batch.from_data_list(batch)\n",
    "\n",
    "def create_dataloaders(df, batch_size=32, train_split=0.8):\n",
    "    \"\"\"Create train and validation dataloaders with optimized settings.\"\"\"\n",
    "    dataset = PolymerDataset(df)\n",
    "    \n",
    "    train_size = int(train_split * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    \n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        dataset, [train_size, val_size]\n",
    "    )\n",
    "    \n",
    "    # Reduce workers to speed up initialization\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True,\n",
    "        collate_fn=custom_collate,\n",
    "        num_workers=2,  # ‚Üê Reduced from 4\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,\n",
    "        collate_fn=custom_collate,\n",
    "        num_workers=2,  # ‚Üê Reduced from 4\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "    \"\"\"\n",
    "    Create train and validation dataloaders.\n",
    "    \"\"\"\n",
    "    dataset = PolymerDataset(df)\n",
    "    \n",
    "    # Train/val split\n",
    "    train_size = int(train_split * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    \n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        dataset, [train_size, val_size]\n",
    "    )\n",
    "    \n",
    "    # Use custom collate function\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True,\n",
    "        collate_fn=custom_collate,  # ‚Üê ADD THIS\n",
    "        num_workers = 4,\n",
    "        pin_memory = True,\n",
    "        persistent_workers = True,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,\n",
    "        collate_fn=custom_collate,  # ‚Üê ADD THIS\n",
    "        num_workers = 4,\n",
    "        pin_memory = True,\n",
    "        persistent_workers = True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66f8ecdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GraphConv, global_mean_pool\n",
    "\n",
    "class PolymerGNN(nn.Module):\n",
    "    \"\"\"\n",
    "    3-layer Graph Convolutional Network for polymer property prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_node_features=5, num_outputs=5, hidden_dim=64, embedding_dim=32):\n",
    "        super(PolymerGNN, self).__init__()\n",
    "        \n",
    "        # Graph convolution layers\n",
    "        self.conv1 = GraphConv(num_node_features, hidden_dim)\n",
    "        self.conv2 = GraphConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GraphConv(hidden_dim, embedding_dim)\n",
    "        \n",
    "        # Batch normalization\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn3 = nn.BatchNorm1d(embedding_dim)\n",
    "        \n",
    "        # Dense layers for property prediction\n",
    "        self.fc1 = nn.Linear(embedding_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, num_outputs)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        # GCN layers with ReLU and batch norm\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Global mean pooling (32-dim embeddings)\n",
    "        embeddings = global_mean_pool(x, batch)\n",
    "        \n",
    "        # Dense layers for final predictions\n",
    "        x = F.relu(self.fc1(embeddings))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def get_embeddings(self, data):\n",
    "        \"\"\"Extract 32-dim molecular embeddings.\"\"\"\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        x = F.relu(self.bn1(self.conv1(x, edge_index)))\n",
    "        x = F.relu(self.bn2(self.conv2(x, edge_index)))\n",
    "        x = F.relu(self.bn3(self.conv3(x, edge_index)))\n",
    "        \n",
    "        return global_mean_pool(x, batch)\n",
    "    \n",
    "    \n",
    "def extract_embeddings_for_xgboost(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Extract 32D embeddings from GNN for all molecules.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    targets = []\n",
    "    masks = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            if batch is None:\n",
    "                continue\n",
    "            batch = batch.to(device)\n",
    "            \n",
    "            # Use get_embeddings() method\n",
    "            emb = model.get_embeddings(batch)  # Shape: [batch_size, 32]\n",
    "            \n",
    "            embeddings.append(emb.cpu().numpy())\n",
    "            targets.append(batch.y.cpu().numpy())\n",
    "            masks.append(batch.y_mask.cpu().numpy())\n",
    "    \n",
    "    embeddings = np.vstack(embeddings)  # [num_samples, 32]\n",
    "    targets = np.vstack(targets)        # [num_samples, 5]\n",
    "    masks = np.vstack(masks)            # [num_samples, 5]\n",
    "    \n",
    "    return embeddings, targets, masks\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Masked MSE Loss\n",
    "# ---------------------------------------------------------\n",
    "def masked_mse_loss(pred, target, mask):\n",
    "    \"\"\"\n",
    "    Compute MSE only over available target values.\n",
    "    pred, target, mask = [batch, 5]\n",
    "    \"\"\"\n",
    "    # Sanity reshape if dataloader flattens targets\n",
    "    if target.dim() == 1:\n",
    "        target = target.view(pred.shape[0], -1)\n",
    "        mask   = mask.view(pred.shape[0], -1)\n",
    "\n",
    "    loss = (pred - target) ** 2\n",
    "    loss = loss * mask\n",
    "    return loss.sum() / mask.sum().clamp(min=1)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Training loop for one epoch\n",
    "# ---------------------------------------------------------\n",
    "def train_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(loader, desc=\"Training\"):\n",
    "        if batch is None:\n",
    "            continue\n",
    "\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(batch)\n",
    "        loss = masked_mse_loss(pred, batch.y, batch.y_mask)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / max(len(loader), 1)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Validation loop\n",
    "# ---------------------------------------------------------\n",
    "def validate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            if batch is None:\n",
    "                continue\n",
    "\n",
    "            batch = batch.to(device)\n",
    "            pred = model(batch)\n",
    "            loss = masked_mse_loss(pred, batch.y, batch.y_mask)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / max(len(loader), 1)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Full Training Procedure\n",
    "# ---------------------------------------------------------\n",
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_epochs=100,\n",
    "    lr=1e-3,\n",
    "    device=None\n",
    "):\n",
    "    # Resolve device\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    print(f\"\\nüöÄ Using device: {device}\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        patience=10,\n",
    "        factor=0.5\n",
    "    )\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "        val_loss   = validate(model, val_loader, device)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch}/{num_epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "        # Save best\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"best_model.pt\")\n",
    "            print(\"üíæ Saved new best model!\")\n",
    "\n",
    "    print(\"\\nüéâ Training complete! Best Val Loss:\", best_val_loss)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d7d301e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SMILESOnlyDataset class defined\n"
     ]
    }
   ],
   "source": [
    "class SMILESOnlyDataset(Dataset):\n",
    "    \"\"\"Dataset for extracting embeddings from SMILES only (no targets needed).\"\"\"\n",
    "    def __init__(self, df):\n",
    "        super().__init__()\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.smiles_column = 'canonical_SMILES'\n",
    "        \n",
    "    def len(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def get(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        smiles = row[self.smiles_column]\n",
    "        graph = smiles_to_graph(smiles)\n",
    "        return graph\n",
    "\n",
    "print(\"‚úÖ SMILESOnlyDataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a4689b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üß¨ PHASE 1: TRAIN GNN ON SMILES ONLY (NO NUMERICAL FEATURES)\n",
      "============================================================\n",
      "üìä Total molecules: 10343\n",
      "üìö Molecules with ‚â•1 target: 7973\n",
      "\n",
      "‚ö†Ô∏è  GNN will ONLY see SMILES ‚Üí Graph structure\n",
      "   NO numerical features used in GNN training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Loq Gaming\\AppData\\Local\\Temp\\ipykernel_18900\\759740579.py:154: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  train_loader = DataLoader(\n",
      "C:\\Users\\Loq Gaming\\AppData\\Local\\Temp\\ipykernel_18900\\759740579.py:163: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  val_loader = DataLoader(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Training batches: 200\n",
      "üì¶ Validation batches: 50\n",
      "\n",
      "üöÄ Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üß¨ PHASE 1: TRAIN GNN ON SMILES ONLY (NO NUMERICAL FEATURES)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Filter data that has at least 1 target\n",
    "target_columns = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "df_with_targets = df_augmented[df_augmented[target_columns].notna().any(axis=1)].copy()\n",
    "\n",
    "print(f\"üìä Total molecules: {len(df_augmented)}\")\n",
    "print(f\"üìö Molecules with ‚â•1 target: {len(df_with_targets)}\")\n",
    "print(f\"\\n‚ö†Ô∏è  GNN will ONLY see SMILES ‚Üí Graph structure\")\n",
    "print(f\"   NO numerical features used in GNN training!\")\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = PolymerGNN(num_node_features=5, num_outputs=5).to(device)\n",
    "\n",
    "# Create dataloaders from molecules with targets\n",
    "# PolymerDataset only uses SMILES (via smiles_to_graph)\n",
    "train_loader, val_loader = create_dataloaders(\n",
    "    df_with_targets, \n",
    "    batch_size=32, \n",
    "    train_split=0.8\n",
    ")\n",
    "\n",
    "print(f\"üì¶ Training batches: {len(train_loader)}\")\n",
    "print(f\"üì¶ Validation batches: {len(val_loader)}\")\n",
    "\n",
    "# Train GNN (learns from graph structure only)\n",
    "model = train_model(model, train_loader, val_loader, num_epochs=50, device=device)\n",
    "\n",
    "print(\"\\n‚úÖ GNN training complete!\")\n",
    "print(\"   GNN learned molecular representations from SMILES graph structure only\")\n",
    "\n",
    "# ================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîÑ PHASE 2: EXTRACT EMBEDDINGS FOR ALL MOLECULES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "model.eval()\n",
    "\n",
    "# Create dataloader for ALL molecules (including those without targets)\n",
    "all_smiles_dataset = SMILESOnlyDataset(df_augmented)\n",
    "all_smiles_loader = DataLoader(\n",
    "    all_smiles_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,  # Keep order!\n",
    "    collate_fn=custom_collate,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"üìä Extracting embeddings for {len(df_augmented)} molecules...\")\n",
    "print(f\"   Using SMILES-only dataset (no targets needed)\")\n",
    "\n",
    "# Extract embeddings\n",
    "embeddings_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(all_smiles_loader, desc=\"Extracting embeddings\"):\n",
    "        if batch is None:\n",
    "            continue\n",
    "        \n",
    "        batch = batch.to(device)\n",
    "        emb = model.get_embeddings(batch)  # [batch_size, 32]\n",
    "        embeddings_list.append(emb.cpu().numpy())\n",
    "\n",
    "# Stack all embeddings\n",
    "all_embeddings = np.vstack(embeddings_list)  # [N, 32]\n",
    "\n",
    "print(f\"‚úÖ Extracted embeddings shape: {all_embeddings.shape}\")\n",
    "assert len(all_embeddings) == len(df_augmented), f\"Embedding count mismatch! {len(all_embeddings)} != {len(df_augmented)}\"\n",
    "\n",
    "# ================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîó PHASE 3: CONCATENATE EMBEDDINGS WITH NUMERICAL FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"üìä Available numerical columns: {len(numerical_columns)}\")\n",
    "if len(numerical_columns) > 0:\n",
    "    print(f\"   Columns: {numerical_columns}\")\n",
    "\n",
    "# Extract numerical features (excluding target columns for features)\n",
    "# We want to use numerical features as INPUT, not as targets\n",
    "if len(numerical_columns) > 0:\n",
    "    # Fill NaN with 0 (or use mean/median imputation)\n",
    "    numerical_features = df_augmented[numerical_columns].fillna(0).values\n",
    "    \n",
    "    # Concatenate: [embeddings | numerical features]\n",
    "    X_combined = np.hstack([all_embeddings, numerical_features])\n",
    "    \n",
    "    print(f\"\\nüìä Combined feature matrix:\")\n",
    "    print(f\"   Embeddings (from SMILES):  {all_embeddings.shape}\")\n",
    "    print(f\"   Numerical features:        {numerical_features.shape}\")\n",
    "    print(f\"   Combined:                  {X_combined.shape}\")\n",
    "    print(f\"\\n   Feature breakdown:\")\n",
    "    print(f\"     - Dimensions 0-31:   GNN embeddings (from SMILES)\")\n",
    "    print(f\"     - Dimensions 32-{X_combined.shape[1]-1}:  Numerical features\")\n",
    "else:\n",
    "    X_combined = all_embeddings\n",
    "    print(f\"‚ö†Ô∏è  No numerical features found\")\n",
    "    print(f\"   Using embeddings only: {X_combined.shape}\")\n",
    "\n",
    "# Extract targets and masks\n",
    "y_all = df_augmented[target_columns].values\n",
    "mask_all = df_augmented[target_columns].notna().astype(float).values\n",
    "y_all = np.nan_to_num(y_all, nan=0.0)\n",
    "\n",
    "print(f\"\\n‚úÖ Target shape: {y_all.shape}\")\n",
    "print(f\"‚úÖ Mask shape: {mask_all.shape}\")\n",
    "\n",
    "# Show target availability\n",
    "print(f\"\\nüìä Target availability:\")\n",
    "for i, col in enumerate(target_columns):\n",
    "    available = mask_all[:, i].sum()\n",
    "    pct = (available / len(mask_all)) * 100\n",
    "    print(f\"   {col:10s}: {int(available):6d} / {len(mask_all):6d} ({pct:5.1f}%)\")\n",
    "\n",
    "# ================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üå≥ PHASE 4: TRAIN XGBOOST ON COMBINED FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Only use samples with at least 1 target\n",
    "has_target = mask_all.sum(axis=1) > 0\n",
    "X_train_combined = X_combined[has_target]\n",
    "y_train_combined = y_all[has_target]\n",
    "mask_train_combined = mask_all[has_target]\n",
    "\n",
    "print(f\"üìä Training data selection:\")\n",
    "print(f\"   Total molecules:         {len(X_combined)}\")\n",
    "print(f\"   With ‚â•1 target:          {len(X_train_combined)}\")\n",
    "print(f\"   Without targets:         {len(X_combined) - len(X_train_combined)}\")\n",
    "\n",
    "print(f\"\\nüìä Feature dimension breakdown:\")\n",
    "print(f\"   Total features:          {X_train_combined.shape[1]}\")\n",
    "print(f\"     - GNN embeddings:      32\")\n",
    "if len(numerical_columns) > 0:\n",
    "    print(f\"     - Numerical features:  {len(numerical_columns)}\")\n",
    "    # Show which numerical features are included\n",
    "    print(f\"\\n   Numerical features used:\")\n",
    "    for col in numerical_columns:\n",
    "        is_target = \" (TARGET)\" if col in target_columns else \" (FEATURE)\"\n",
    "        print(f\"     - {col}{is_target}\")\n",
    "\n",
    "# Train/val split\n",
    "X_train, X_val, y_train, y_val, mask_train, mask_val = train_test_split(\n",
    "    X_train_combined,\n",
    "    y_train_combined,\n",
    "    mask_train_combined,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nüì¶ Data split:\")\n",
    "print(f\"   Train set: {len(X_train)} samples\")\n",
    "print(f\"   Val set:   {len(X_val)} samples\")\n",
    "\n",
    "# Train XGBoost\n",
    "base_xgb = XGBRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_model = MultiOutputRegressor(base_xgb)\n",
    "\n",
    "print(\"\\nüå≥ Training XGBoost on combined features...\")\n",
    "print(f\"   Input: [{all_embeddings.shape[1]} embeddings + {X_combined.shape[1] - all_embeddings.shape[1]} numerical]\")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "print(\"‚úÖ XGBoost training complete!\")\n",
    "\n",
    "# ================================================================\n",
    "# Evaluate\n",
    "y_val_pred = xgb_model.predict(X_val)\n",
    "\n",
    "property_names = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä XGBOOST PERFORMANCE (Embeddings + Numerical Features)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, prop in enumerate(property_names):\n",
    "    mask_idx = mask_val[:, i] == 1\n",
    "    \n",
    "    if mask_idx.sum() > 0:\n",
    "        y_true = y_val[mask_idx, i]\n",
    "        y_pred = y_val_pred[mask_idx, i]\n",
    "        \n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        \n",
    "        print(f\"{prop:10s} | RMSE: {rmse:8.4f} | R¬≤: {r2:7.4f} | Samples: {mask_idx.sum():5d}\")\n",
    "    else:\n",
    "        print(f\"{prop:10s} | No validation samples\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üíæ PHASE 5: SAVE RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs('../data', exist_ok=True)\n",
    "\n",
    "# Save combined features\n",
    "np.save('../data/combined_features.npy', X_combined)\n",
    "np.save('../data/targets.npy', y_all)\n",
    "np.save('../data/masks.npy', mask_all)\n",
    "\n",
    "# Save embeddings separately\n",
    "np.save('../data/molecule_embeddings.npy', all_embeddings)\n",
    "\n",
    "# Save XGBoost model\n",
    "import joblib\n",
    "joblib.dump(xgb_model, '../data/xgboost_combined_model.pkl')\n",
    "\n",
    "# Save feature metadata\n",
    "feature_metadata = {\n",
    "    'embedding_dim': all_embeddings.shape[1],\n",
    "    'numerical_features': numerical_columns,\n",
    "    'total_features': X_combined.shape[1],\n",
    "    'target_columns': target_columns\n",
    "}\n",
    "joblib.dump(feature_metadata, '../data/feature_metadata.pkl')\n",
    "\n",
    "print(\"‚úÖ Saved:\")\n",
    "print(\"   - combined_features.npy         (embeddings + numerical)\")\n",
    "print(\"   - targets.npy                   (target values)\")\n",
    "print(\"   - masks.npy                     (availability mask)\")\n",
    "print(\"   - molecule_embeddings.npy       (32D embeddings only)\")\n",
    "print(\"   - xgboost_combined_model.pkl    (trained model)\")\n",
    "print(\"   - feature_metadata.pkl          (feature information)\")\n",
    "\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"   Total molecules:        {len(df_augmented)}\")\n",
    "print(f\"   Embedding dimension:    32\")\n",
    "print(f\"   Numerical features:     {len(numerical_columns)}\")\n",
    "print(f\"   Combined features:      {X_combined.shape[1]}\")\n",
    "print(f\"   Molecules with targets: {has_target.sum()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep Learning Env",
   "language": "python",
   "name": "dl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
